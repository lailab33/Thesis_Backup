---
title: "Thesis Notebook"
output:
  html_document:
    df_print: paged
---

libraries
  load once per session
```{r message=FALSE}
#loading the necesary packages for this code
library(dplyr)
library(tidyr)
library(ggplot2)
library(readr)
library(stringr)
library(skimr)
library(splitstackshape)
library(janitor)
library(jcolors)
library(wordcloud)
library(wordcloud2)
library(tm)
library(data.table)
library(utils)
library(reshape2)

#quanteda packages
library(Rcpp) #rcpp has to load before quanteda
library(quanteda)
library(readtext)
library(devtools)
library(quanteda.textmodels)

library(spacyr)
library(newsmap)
```

Strigning the different databases into one
```{r}
#loading the csv files and then joining them in to one data base
  ##converting blanks in to NA

#csv file with data from 1972-2017
ev_1 <- read.csv("ev_1.csv", na.strings = c("", "NA"))

#csv file with data from 2018-2020
ev_2 <- read.csv("ev_2.csv", na.strings = c("", "NA"))

#csv file with data from 2021
ev_3 <- read.csv("ev_3.csv", na.strings = c("", "NA"))

#loading the packages all together
data.loaded <- rbind(ev_1, ev_2, ev_3)

#creating a .csv file from table with all of the databases bound together to keep environment clean through the project
write_csv(data.loaded, "data.loaded.csv")
```

Cleaning the data
```{r}
#loading the csv file with all the databases in to a data frame
data.original <- read.csv("data.loaded.csv")

#cleaning the csv file for better usage in this project

#discarding the columns that won't be used for the project
data.original <- data.original %>%
 select(-Art..No., -Molecular.Sequence.Numbers, -Chemicals.CAS,-Tradenames, -Manufacturers, -Funding.Text.1, -Funding.Text.2, -Funding.Text.3, -Funding.Text.4, -Funding.Text.5, -Funding.Text.6, -Funding.Text.7, -Funding.Text.8, -Funding.Text.9, -Funding.Text.10, -Correspondence.Address,  -Abbreviated.Source.Title)

#reaming the columns for a better workflow
names(data.original) <- c("authors", "author.id", "title", "year", "source.title", "volume", "issue", "page.start", "page.end", "page.count", "cited.by", "doi", "link", "affiliations", "author.w.affiliations", "abstract", "author.keywords", "index.keywords", "funding.details", "references", "editors", "sponsors", "publisher", "conference.name", "conference.date", "conference.location", "conference.code", "issn", "isbn", "coden", "pubmed.id", "original.language", "document.type", "publication.stage", "open.access", "source", "eid")

#discarding any repeated rows (ie: documents) that might have shown up
data <- unique(data.original)

#checking which columns use "[No x available]" format to replace that message with "NA" instead for better workflow later
  ##those columns that use it have exact language listed in comments next to it
data %>% filter(grepl("available]", authors)) #[No author name available]
data %>% filter(grepl("available]", author.id)) #[No author id available]
data %>% filter(grepl("available]", title))
data %>% filter(grepl("available]", year))
data %>% filter(grepl("available]", source.title))
data %>% filter(grepl("available]", volume))
data %>% filter(grepl("available]", issue))
data %>% filter(grepl("available]", page.start))
data %>% filter(grepl("available]", page.end))
data %>% filter(grepl("available]", page.count))
data %>% filter(grepl("available]", cited.by))
data %>% filter(grepl("available]", doi))
data %>% filter(grepl("available]", link))
data %>% filter(grepl("available]", affiliations))
data %>% filter(grepl("available]", author.w.affiliations))
data %>% filter(grepl("available]", abstract)) #[No abstract available]
data %>% filter(grepl("available]", author.keywords))
data %>% filter(grepl("available]", index.keywords))
data %>% filter(grepl("available]", funding.details))
data %>% filter(grepl("available]", references))
data %>% filter(grepl("available]", editors))
data %>% filter(grepl("available]", sponsors))
data %>% filter(grepl("available]", publisher))
data %>% filter(grepl("available]", conference.name))
data %>% filter(grepl("available]", conference.date))
data %>% filter(grepl("available]", conference.location))
data %>% filter(grepl("available]", conference.code))
data %>% filter(grepl("available]", issn))
data %>% filter(grepl("available]", isbn))
data %>% filter(grepl("available]", coden))
data %>% filter(grepl("available]", pubmed.id))
data %>% filter(grepl("available]", original.language))
data %>% filter(grepl("available]", document.type))
data %>% filter(grepl("available]", publication.stage))
data %>% filter(grepl("available]", open.access))
data %>% filter(grepl("available]", source))
data %>% filter(grepl("available]", eid))

#replacing text about missing data for NA instead in the columns that use text to indicate missing information
data <- data %>%
  mutate(authors = na_if(authors, "[No author name available]")) %>%
  mutate(author.id = na_if(author.id, "[No author id available]")) %>%
  mutate(abstract = na_if(abstract, "[No abstract available]"))

##checking if any columns are totally empty
  ##almost totally empty: editors (5 not empty), sponsors (4 not empty), conference.name (9 not empty), conference.date (9 not empty), conference location (2 not empty), conference code (8 not empty), page.count (13 not empty)
colSums(is.na(data))

#discarding columns that are totally empty or that are empty except for one row, as they will not provide useful information in the aggregate
  #adding an article ID to each column
data <- data %>%
 select(- editors, -sponsors, -conference.name, -conference.date, -conference.location, -conference.code, -isbn, -page.count) %>%
  mutate(id = row_number())

```

Writing data csv file to save time during loading
```{r}
#writing a csv file of data to use in the shiny apps
write_csv(data, "data.csv")
```

Loading the csv file so pervious process does not have to be redone
```{r}
#loading the data back in to this workplace so the data doesn't have to run every time
data <- read.csv("data.csv")
```


```{r} 
#obtaining the articles that are represented in the database
  #total of 719 journals represented
journals <- unique(data[c("source.title")])

#obtaining the years that are represented in the database: 1972 - 2021
min(data$year)
max(data$year)

#finding the earliest instance by each journal
journal.first.instance <- data %>%
  group_by(source.title) %>%
  filter(year == min(year)) %>%
  select(year, source.title) %>%
  distinct(year)

write_csv(journal.first.instance, "journals_by_year.csv")
```

```{r}
#seeing where the keywords of ecological validity are found for each article

data.location <- data %>%
  #looking at the keywords in the title, giving a true if the keywords are there and a false there are no keywords
  mutate(
    title.ev = case_when(
    (grepl("ecological validity|ecologically valid|ecologically-valid|ecological validities", title, ignore.case = TRUE)) ~ TRUE,
    TRUE ~ FALSE
    )
  ) %>%
  #looking at the keywords in the abstract
  mutate(
    abstract.ev = case_when(
    (grepl("ecological validity|ecologically valid|ecologically-valid|ecological validities", abstract, ignore.case = TRUE)) ~ TRUE,
    TRUE ~ FALSE
    )
  ) %>%
  mutate(
    #looking at the author keywords
    index.keywords.ev = case_when(
    (grepl("ecological validity|ecologically valid|ecologically-valid|ecological validities", index.keywords, ignore.case = TRUE)) ~ TRUE,
    TRUE ~ FALSE
    )
  ) %>%
  mutate(
    #looking at the author keywords
    author.keywords.ev = case_when(
    (grepl("ecological validity|ecologically valid|ecologically-valid|ecological validities", author.keywords, ignore.case = TRUE)) ~ TRUE,
    TRUE ~ FALSE
    )
  ) %>%
  mutate(
    #looking at both of the keywords in to one category
    keywords.all.ev = case_when(
    (grepl("ecological validity|ecologically valid|ecologically-valid|ecological validities", index.keywords, ignore.case = TRUE)) ~ TRUE,
    (grepl("ecological validity|ecologically valid|ecologically-valid|ecological validities", author.keywords, ignore.case = TRUE)) ~ TRUE,
    TRUE ~ FALSE
    )
  )

```


```{r}
#checking over articles that didn't have the keywords in the title, keywords or abstract

data.location.none <- data.location %>%
  filter(title.ev == FALSE) %>%
  filter(abstract.ev == FALSE) %>%
  filter(keywords.all.ev == FALSE)

#write_csv(data.location.none, "data.location.none.csv")

#run with just keywords:ecological validity|ecologically valid
  #id: 63, says "ecologically-valid" in the abstract
  #id 137, says "ecological validities" in the abstract

#run with keywords: ecological validity|ecologically valid|ecologically-valid|ecological validities

#ids: 1025, 1332, 1342 & 1980 were corrected by hand after exporting the csv file to look over personally (all had keywords in the abstract and I corrected by hand before loading again)
  #id: 1025, uses [" ecological" validity]
  #id: 1332 & 1342, uses [(ecological) validity]
  #id 1980, uses (ecological) validity

#loading the csv file that was corrected by hand
data.location.none.corrected <- read.csv("data.location.none.csv")

#filtering out the un-corrected ones from the original database
data.location <- data.location %>%
  filter(!(id %in% data.location.none.corrected$id)) #filtering out those articles that I corrected by hand

#adding my hand corrected rows to the database
data.location <- rbind(data.location.none.corrected, data.location)

```

```{r}
## that the location of the keywords have been accounted for in all of the articles in the database
data.location.check <- data.location %>%
  filter(title.ev == FALSE) %>%
  filter(abstract.ev == FALSE) %>%
  filter(keywords.all.ev == FALSE)

data <- data.location
```


```{r}
#giving each author.id, author name, affilication, index.keywords, references, and author.keywords a separate column for each piece of distinct data
data <- cSplit(data, "author.id", sep=";")

data <-  cSplit(data, "authors", sep=",")
  
data <-  cSplit(data, "affiliations", sep=";")

data <- cSplit(data, "index.keywords", sep=";")
  
data <- cSplit(data, "references", sep=";")

data <- cSplit(data, "author.keywords", sep=";")

#writing a csv document for the shiny apps
write_csv(data, "data.split.csv")
```


```{r}
#creating an aesthetic for the visualizations
theme_thesis <- theme_bw() +
  theme(
  axis.text = element_text(family = "Calibri"),
  axis.title = element_text(face = "bold", family = "Calibri")
  )
```


```{r}
#making a graph of what year journals began to publish about ecological validity

#counting number of journals with articles first published by year
journal.first.instance.graph <- journal.first.instance %>%
  ungroup() %>%
  select(year) %>%
  count(year)

  ggplot(journal.first.instance.graph) +
  geom_line(aes(x=year, y=n), color = "#57837B") +  
    scale_x_continuous(breaks = c(1970, 1975, 1980, 1985, 1990, 1995, 2000, 2005, 2010, 2015, 2020)) +
    labs(title= "First year journals published articles about Ecological Validity", x = "Number of journals", y = "Year first EV article was published") +
    theme_thesis

```

```{r}
#graph of distribution by year

#filtering just the year data and counting how many articles there are per year
data.by.year <- data %>%
  select(year) %>%
  count(year)

#creating the graph of the counts per year
ggplot(data.by.year) +
  geom_line(aes(x=year, y=n), color = "#57837B") +
  scale_x_continuous(breaks = c(1970, 1975, 1980, 1985, 1990, 1995, 2000, 2005, 2010, 2015, 2020)) +
  scale_y_continuous(breaks = c(0, 25, 50, 75, 100, 125, 150, 175, 200, 225, 250)) +
  labs(title= "Number of articles about EV per year in all journals", x = "Year", y = "Number of journals") +
  theme_thesis

```

```{r}
#histogram of how many citations EV items recieved
  ##277 rows were removed by R for "containign non-finite values"
  ## bin width of 30
ggplot(data, aes(x=cited.by)) +
  geom_histogram(fill = "#57837B") +
  labs(title= "Histogram of the number of citations recieved by EV items", x = "Number of citations", y = "Number of articles") +
  theme_thesis
```


```{r}
#checking how many articles are missing abstracts
  #total articles missing abstracts = 
data.no.abstract <- data %>%
  filter(is.na(abstract)) %>% #filtering those rows in abstract that have NA
  mutate(has.abstract = "No")

data.yes.abstract <- data %>%
  filter(!(id %in% data.no.abstract$id)) %>% #filtering out those articles already categorized as NA
  mutate(has.abstract = "Yes")

data.abstract.both <- rbind(data.no.abstract, data.yes.abstract)

#stacked bar graph of whether different kinds of items have abstracts listed or not
ggplot(data.abstract.both, aes(x=document.type, fill=has.abstract)) +
  geom_bar() +
  labs(title= "Distribution of document types about EV and wether they have abstracts", x = "Kinds of documents", y = "Number of documents") +
  guides(fill = guide_legend(title = "Has Abstract")) +
  scale_fill_manual(values = c("#ECE7B4", "#32502E")) +
  theme_thesis +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

```{r}
#pie graph of whether the items have abstracts or not
data.abstract.both.pie <- data.abstract.both %>%
  count(has.abstract) %>%
  mutate(per = (100*n)/2650)

#rounding the percentage
data.abstract.both.pie$per <- round(data.abstract.both.pie$per)

#making the graph
ggplot(data.abstract.both.pie, aes(x="", y = per, fill = has.abstract)) +
  geom_bar(stat="identity", width=1) +
  coord_polar("y", start=0) + 
  theme_void()+
  geom_text(aes(label = paste0(per,"%")), position = position_stack(vjust=0.5)) +
    guides(fill = guide_legend(title = "Has Abstract")) +
  labs(title = "Percentages of items with or without abstracts") +
  scale_fill_manual(values = c("#F1ECC3", "#57837B"))

```

marimo list of stopwords: https://github.com/koheiw/marimo/blob/master/yaml/stopwords_en.yml

```{r}
#title word cloud 

#trying to use quanteda to make wordclouds
data.title <- data %>%
  select(title)

#loading just the titles as part of the main corpus
title.corpus <- corpus(data.title, text_field = "title")

#removing punctuation from the title
  #removing "stopwords" (common grammatical words in the english language, which are here; https://github.com/koheiw/marimo/blob/master/yaml/stopwords_en.yml )
wc.title <- tokens(title.corpus, remove_punct = TRUE) %>%
    tokens_remove(pattern = stopwords("en"))

#creating a document-feature matrix (DFM) from the tokens object
wc.title.dfm <- dfm(wc.title)

#creating a word cloud of 100 words from the dfm
  #setting the seed for reproducibility
set.seed(132)
quanteda.textplots::textplot_wordcloud(wc.title.dfm, max_words = 200,
                                       color = c("#515E63", "#5B7DB1", "#57837B", "#32502E"))

#dataframe of the most used words in the titles w/ number of times used
title_freq <- quanteda.textstats::textstat_frequency(wc.title.dfm)

```


```{r}
#abstract word cloud 

#trying to use quanteda to make wordclouds
data.abstract <- data %>%
  select(abstract)

#loading just the titles as part of the main corpus
abstract.corpus <- corpus(data.abstract, text_field = "abstract")

#removing punctuation from the title
  #removing "stopwords" (common grammatical words in the english language, which are here; https://github.com/koheiw/marimo/blob/master/yaml/stopwords_en.yml )
wc.abstract <- tokens(abstract.corpus, remove_punct = TRUE) %>%
  tokens_remove(pattern = stopwords("en"))

#creating a document-feature matrix (DFM) from the tokens object
wc.abstract.dfm <- dfm(wc.abstract)

#creating a word cloud of 100 words from the dfm
  #setting the seed for reproducibility
set.seed(132)
quanteda.textplots::textplot_wordcloud(wc.abstract.dfm, max_words = 200,
                                       color = c("#515E63", "#5B7DB1", "#57837B", "#32502E"))

#creating a list of the most used words w/number of times used
abstract_freq <- quanteda.textstats::textstat_frequency(wc.abstract.dfm)
```

```{r}
#keyword word cloud: just by the word in the keywords

#trying to use quanteda to make wordclouds
data.all.keywords <- data %>%
  select(index.keywords_01:index.keywords_90, author.keywords_01:author.keywords_19, id)

data.all.keywords <-  melt(data.all.keywords, id.vars=c("id"), na.rm = TRUE) %>%
  select(-variable)

#loading just the titles as part of the main corpus
keywords.corpus <- corpus(data.all.keywords, text_field = "value")

#removing punctuation from the title
  #removing "stopwords" (common grammatical words in the english language, which are here; https://github.com/koheiw/marimo/blob/master/yaml/stopwords_en.yml )
wc.all.keywords <- tokens(keywords.corpus, remove_punct = TRUE) %>%
  tokens_remove(pattern = stopwords("en"))

#creating a document-feature matrix (DFM) from the tokens object
wc.all.keywords.dfm <- dfm(wc.all.keywords)

#creating a word cloud of 100 words from the dfm
  #setting the seed for reproducibility
set.seed(132)
quanteda.textplots::textplot_wordcloud(wc.all.keywords.dfm, max_words = 200,
                                       color = c("#515E63", "#5B7DB1", "#57837B", "#32502E"))

#creating a list of the most used words w/number of times used
all.keywords.dfm <- quanteda.textstats::textstat_frequency(wc.all.keywords.dfm)

```

```{r}
#wordcloud of the dataframe 

#making a dataframe listing all the keywords
data.all.keywords.unit <- data %>%
  select(index.keywords_01:index.keywords_90, author.keywords_01:author.keywords_19, id)

data.all.keywords.unit <-  melt(data.all.keywords.unit, id.vars=c("id"), na.rm = TRUE) %>%
  select(-variable)

#making everything lowercase
data.all.keywords.unit$value = tolower(data.all.keywords.unit$value)

#selecting only the keywords
data.all.keywords.unit <- data.all.keywords.unit %>%
  select(-id) %>%
  drop_na(value) %>%
  count(value)

#making the wordcloud
wordcloud(words = data.all.keywords.unit$value, freq = data.all.keywords.unit$n,
          max.words=200, random.order=FALSE, rot.per=0.35,
          colors=brewer.pal(8, "Dark2"), scale=c(2.0,0.25))
```


```{r}
#information about number of articles indexed in scopus by year, sourced from the extra materials of:
  #Thelwall, M. (2022) Scopus 1900-2020: Growth in articles, abstracts, countries, fields, and journals. Quantitative Science Studies. Advance Publication. https://doi.org/10.1162/qss_a_00177

scopus.psych <- data.frame(year = c(1900,	1901,	1902,	1903,	1904,	1905,	1906,	1907,	1908,	1909,	1910,	1911,	1912,	1913,	1914,	1915,	1916,	1917,	1918,	1919,	1920,	1921,	1922,	1923,	1924,	1925,	1926,	1927,	1928,	1929,	1930,	1931,	1932,	1933,	1934,	1935,	1936,	1937,	1938,	1939,	1940,	1941,	1942,	1943,	1944,	1945,	1946,	1947,	1948,	1949,	1950,	1951,	1952,	1953,	1954,	1955,	1956,	1957,	1958,	1959,	1960,	1961,	1962,	1963,	1964,	1965,	1966,	1967,	1968,	1969,	1970,	1971,	1972,	1973,	1974,	1975,	1976,	1977,	1978,	1979,	1980,	1981,	1982,	1983,	1984,	1985,	1986,	1987,	1988,	1989,	1990,	1991,	1992,	1993,	1994,	1995,	1996,	1997,	1998,	1999,	2000,	2001,	2002,	2003,	2004,	2005,	2006,	2007,	2008,	2009,	2010,	2011,	2012,	2013,	2014,	2015,	2016,	2017,	2018,	2019,	2020),
                           articles = c(44,	33,	44,	32,	69,	57,	53,	54,	93,	84,	81,	92,	100,	108,	119,	104,	108,	105,	84,	104,	95,	62,	109,	105,	96,	80,	138,	134,	151,	123,	149,	133,	142,	137,	138,	141,	161,	232,	218,	179,	186,	184,	145,	182,	171,	186,	171,	192,	219,	294,	368,	362,	407,	444,	422,	351,	340,	325,	369,	359,	304,	341,	417,	437,	537,	1054,	1079,	1289,	1436,	1163,	1470,	1526,	1478,	1855,	2025,	1942,	2101,	2139,	2411,	2346,	2490,	2659,	2806,	2804,	2711,	2799,	2748,	2789,	3009,	3007,	3347,	3352,	3551,	3490,	3540,	3815,	5051,	4957,	5260,	5189,	5363,	5521,	5245,	4746,	5531,	5331,	6832,	7873,	8213,	7032,	7448,	7778,	8246,	8949,	10238,	10324,	11584,	12028,	12092,	13691,	15634))
```

```{r}
#graph of the number of articles indexed by Scopus as calculated from Thelwall (2022)
scopus.psych <- scopus.psych %>%
  filter(year > 1969)

ggplot(scopus.psych) +
  geom_line(aes(x=year, y=articles), color = "#57837B") +
  scale_x_continuous(breaks = c(1970, 1975, 1980, 1985, 1990, 1995, 2000, 2005, 2010, 2015, 2020)) +
  #scale_y_continuous(breaks = c(0, 25, 50, 75, 100, 125, 150, 175, 200, 225, 250)) +
  labs(title= "Number of psyschology articles indexed by Scopus (Thelwall 2022)", x = "Year", y = "Number of articles") +
  theme_thesis


```
```{r}
#Creating percentage of what number of articles of Scopus indexing systems are about ecological validity

#removing year 1970 and 1971 from the scopus database
scopus.psych.n <- scopus.psych %>%
  filter(year != 1970) %>%
  filter(year != 1971)

#removing year 2021 from ev database (scopus doesn't have it)) + adding year 1974, which doesn't exist in the ev database (aka there are no ev articles in 1974)
#creating the row with information 
data.by.year.new.row <- data.frame(year = c(1974), n =c(0))

#binding new row to updated databse
data.by.year.updated <- rbind(data.by.year, data.by.year.new.row)

#removing the 2021 year and arranging from lowest year to highest
data.by.year.scopus <- data.by.year.updated %>%
  filter(year != 2021) %>%
  arrange(year) %>%
  select(-year)

#renaming thelwall scopus information to bind
names(scopus.psych.n) <- c("year", "total.n")

#binding the information by year about ev articles and thelwall's information on total psych articles by year
data.year.scopus.psych <- cbind(scopus.psych.n, data.by.year.scopus)

#creating the percentage information
data.year.scopus.psych <- data.year.scopus.psych %>%
  mutate(per = ((n*100)/total.n))

```

```{r}
#line graph of what percentage of information are about ev
ggplot(data.year.scopus.psych, aes(x= year, y= per)) +
  geom_line(color = "#57837B") +
  geom_point(color = "#57837B") +
  labs(title= "Percentage of articles indexed in Scopus that are about EV 
       (as per Thelwall et. all's 2021 data)") +
  theme_thesis
```


```{r}
#loading the csv file on number of articles about number of articles scopus has indexed for each year
  #this information is from ME as seen by a scopus search in Feb 2022
psych.articles.year <- read.csv("articles.by.year.csv", na.strings = c("", "NA"))

#making a graph of number of journals indexed by Scopus under the category of psychology
  ggplot(psych.articles.year, aes(x = year, y = article.count)) +
  geom_line(color = "#57837B") +  
    scale_x_continuous(breaks = c(1940, 1945, 1950, 1955, 1960, 1965, 1970, 1975, 1980, 1985, 1990, 1995, 2000, 2005, 2010, 2015, 2020)) +
    labs(title= "Number of journals indexed by Scopus under the category of psychology 
         (Barcenas Meade, Scopus Feb 2022)", x = "Year", y = "Number of journals") +
    theme_thesis

```
```{r}
#making percentage graph with personally gathered data

#filtering to match the years I have EV data for
psych.articles.year.per <- psych.articles.year %>%
  filter(year >= 1972) %>%
  arrange(year)

#arrange from smallest to largest year (data of ev by year)
data.by.year.updated <- data.by.year.updated %>%
  arrange(year) %>%
  select(-year)

#renaming columns for distinguishing ease when bound together
names(psych.articles.year.per) <- c("year", "total.n")

#binding by year items of EV and 
data.meade.psych <- cbind(psych.articles.year.per, data.by.year.updated)

#creating the percentage information
#creating the percentage information
data.meade.psych.per <- data.meade.psych %>%
  mutate(per = ((n*100)/total.n))
```

```{r}
#line graph of what percentage of information are about ev from meade.data

ggplot(data.meade.psych.per, aes(x= year, y= per)) +
  geom_line(color = "#57837B") +
  geom_point(color = "#57837B") +
  labs(title= "Percentage of articles indexed in Scopus that are about EV 
       (as per personal data, Feb 2022)") +
  theme_thesis
```


```{r}
#loading information from Scopus representative of the number of articles per particular journal that are uploaded to scopus January-February 2022

scopus.journal.counts <- read.csv("scopus_journal_counts.csv", na.strings = c("", "NA"))

scopus.source.list <- read.csv("scopus_source_list.csv", na.strings = c("", "NA"))

#cleaning up the dataframe
names(scopus.journal.counts) <- c("journal_title", 1942,	1943,	1944,	1945,	1946,	1947,	1948,	1949,	1950,	1951,	1952,	1953,	1954,	1955,	1956,	1957,	1958,	1959,	1960,	1961,	1962,	1963,	1964,	1965,	1966,	1967,	1968,	1969,	1970,	1971,	1972,	1973,	1974,	1975,	1976,	1977,	1978,	1979,	1980,	1981,	1982,	1983,	1984,	1985,	1986,	1987,	1988,	1989,	1990,	1991,	1992,	1993,	1994,	1995,	1996,	1997,	1998,	1999,	2000,	2001,	2002,	2003,	2004,	2005,	2006,	2007,	2008,	2009,	2010,	2011,	2012,	2013,	2014,	2015,	2016,	2017,	2018,	2019,	2020, 2021, "total.by.journal")
  
```

```{r}
#making a graph of the information from the scopus rep: need to convert data to long and select only the total of the years

#filtering the data to just have the grand total of the information
scopus.total.counts <- scopus.journal.counts  %>%
  select(-total.by.journal) %>%
  filter(journal_title == "Grand Total") %>%
  select(-journal_title)

#converting from wide to long
grand.total.scopus <- as.data.frame(t(scopus.total.counts))

#making the rowname (year) a column
grand.total.scopus <- tibble::rownames_to_column(grand.total.scopus, "year")

#renaming the columns for easier management
names(grand.total.scopus) <- c("year", "total.articles")
  
```

```{r}
#making a graph of the information from the scopus representative
#loading the csv file on number of articles about number of articles scopus has indexed for each year
  #this information is from ME as seen by a scopus search in Feb 2022

#making a graph of number of journals indexed by Scopus under the category of psychology

ggplot(grand.total.scopus, aes(x=year, y=total.articles, group = 1)) +
  geom_line(color = "#57837B") +
  scale_x_discrete(breaks = c(1940, 1945, 1950, 1955, 1960, 1965, 1970, 1975, 1980, 1985, 1990, 1995, 2000, 2005, 2010, 2015, 2020)) +
  #scale_y_discrete(breaks = c(1000, 15000, 20000, 25000, 30000, 35000, 40000, 45000, 50000, 55000, 60000, 65000)) +
  labs(title= "Number of psyschology articles indexed by Scopus (Scopus Rep, Jan-Feb 2022)", x = "Year", y = "Number of articles") +
  theme_thesis +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

  
```

```{r}
#making percentage graph with personally gathered data

#filtering to match the years I have EV data for
grand.total.scopus.per <- grand.total.scopus %>%
  filter(year >= 1972) %>%
  arrange(year)

#renaming columns for distinguishing ease when bound together
names(grand.total.scopus.per) <- c("year", "total.n")

#binding by year items of EV and 
data.scopus.rep <- cbind(grand.total.scopus.per, data.by.year.updated)

#creating the percentage information
#creating the percentage information
data.scopus.rep.per <- data.scopus.rep %>%
  mutate(per = ((n*100)/total.n))
```

```{r}
#line graph of what percentage of information are about ev from scopus representative data
ggplot(data.scopus.rep.per, aes(x=year, y=per, group = 1)) +
  geom_line(color = "#57837B") +
  geom_point(color = "#57837B") +
  labs(title= "Percentage of articles indexed in Scopus that are about EV 
       (Scopus Representative, Jan-Feb 2022)", x = "Year", y = "Percentage of items about EV") +
  scale_x_discrete(breaks = c(1940, 1945, 1950, 1955, 1960, 1965, 1970, 1975, 1980, 1985, 1990, 1995, 2000, 2005, 2010, 2015, 2020)) +
  theme_thesis

```


```{r}
#preparing the dataframe for making the pie chart
data.location.per <- data.location %>%
  count(title.ev, abstract.ev, keywords.all.ev) %>%
  mutate(per = (100*n)/2650)

data.location.labels <- c("None", "Just Keywords", "Just Abstract", "Abstract and Keywords", "Just Title", "Title and Keywords", "Title and Abstract", "Title, Abstract, and Keywords")

data.location.bar <- cbind(data.location.per, data.location.labels)

ggplot(data.location.bar, aes(x = reorder(data.location.labels, n), y = n)) +
  geom_col(fill = "#57837B")  +
  labs(title= "Where EV keywords are in the dataset", x = "Number of articles", y = "Location of keywords") +
  #scale_fill_manual(values = c("#ECE7B4")) +
  theme_thesis +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

```{r}
#making a graph of what the proportions are of having abstract/not having abstract in the data by year
ggplot(data.abstract.both, aes(x=year, fill=has.abstract)) +
  geom_bar() +
  labs(title= "Distribution of document types about EV and wether they have abstracts by year", x = "Year", y = "Number of documents") +
  guides(fill = guide_legend(title = "Has Abstract")) +
  scale_fill_manual(values = c("#ECE7B4", "#32502E")) +
  theme_thesis +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```


```{r}
#making a graph of what the proportions are of having abstract/not having abstract in the data by year
data.abstract.limit <- data.abstract.both %>% 
  filter(year < 2000)

ggplot(data.abstract.limit, aes(x=year, fill=has.abstract)) +
  geom_bar() +
  labs(title= "Distribution of document types about EV and wether they have abstracts by year (1972-2000)", x = "Year", y = "Number of documents") +
  guides(fill = guide_legend(title = "Has Abstract")) +
  theme_thesis +
  scale_fill_manual(values = c("#ECE7B4", "#32502E")) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

```{r}
#check w/Thelwall about his information of citation data
thelwall.all.abstracts <- read.csv("thelwall.all.abstracts.csv")

#renaming the colummns for ease of work
names(thelwall.all.abstracts) <- c("year", "all.articles.left", "all.articles.left.duplicates", "500.abstracts.left", "all.articles.right", "all.articles.right.duplicates", "500.abstracts", "1000.abstracts", "2000.abstracts", "500.abstracts.per", "1000.abstracts.per", "2000.abstracts.per")


```


```{r}
#making a line graph of the percentages of articles in all of scopus that contain abstracts with different numbers of characters
thelwall.all.abstracts.graph <- thelwall.all.abstracts %>%
  select(year, "500.abstracts.per", "1000.abstracts.per", "2000.abstracts.per")

names(thelwall.all.abstracts.graph) <- c("year",  "fiveh.abstracts.per", "tenh.abstracts.per", "twentyh.abstracts.per")

#removing na rows
na.omit(thelwall.all.abstracts.graph) 

#removing the % sign from the rows in the dataframe + converting to numeric values
#500 column
thelwall.all.abstracts.graph$fiveh.abstracts.per = as.numeric(gsub("\\%", "", thelwall.all.abstracts.graph$fiveh.abstracts.per))
as.numeric(thelwall.all.abstracts.graph$fiveh.abstracts.per)

#1000 column
thelwall.all.abstracts.graph$tenh.abstracts.per = as.numeric(gsub("\\%", "", thelwall.all.abstracts.graph$tenh.abstracts.per))
as.numeric(thelwall.all.abstracts.graph$tenh.abstracts.per)

#2000 column
thelwall.all.abstracts.graph$twentyh.abstracts.per = as.numeric(gsub("\\%", "", thelwall.all.abstracts.graph$twentyh.abstracts.per))
as.numeric(thelwall.all.abstracts.graph$twentyh.abstracts.per)

#line graph
ggplot(thelwall.all.abstracts.graph, aes(x = year)) +
  geom_line(aes(y = fiveh.abstracts.per, color = "500 characters")) + 
 geom_line(aes(y = tenh.abstracts.per, color = "1000 characters")) +
  geom_line(aes(y = twentyh.abstracts.per, color = "2000 characters")) + 
  scale_color_manual('Number of characters in abstract', values=c('#F1ECC3', '#C9D8B6', "#57837B")) +
  labs(title= "Number of items with abstracts of different character lengths indexed in Scopus 1900-2020 (Thelwall et al. 2021)", x = "Year", y = "Percentage of articles with abstracts") +
  xlim(1900, 2020) +
  ylim(0,100) +
  theme_thesis

```


```{r}
thelwall.limit.abstracts <- thelwall.all.abstracts.graph %>%
  filter(year > 1941, year < 2000)

ggplot(thelwall.limit.abstracts, aes(x = year)) +
  geom_line(aes(y = fiveh.abstracts.per, color = "500 characters")) + 
 geom_line(aes(y = tenh.abstracts.per, color = "1000 characters")) +
  geom_line(aes(y = twentyh.abstracts.per, color = "2000 characters")) + 
  scale_color_manual('Number of characters in abstract', values=c('#F1ECC3', '#C9D8B6', "#57837B")) +
  labs(title= "Number of items with abstracts of different character lengths indexed in Scopus 1940-2020 (Thelwall et al. 2021)", x = "Year", y = "Percentage of articles with abstracts") +
  xlim(1940, 2001) +
  ylim(0,100) +
  theme_thesis
```

```{r}
#counting number of articles that have abstracts per year
data.abstract.both.per <- data.abstract.both %>%
  select(id, year, has.abstract) %>%
  group_by(year) %>%
  count(has.abstract) %>%
  ungroup() %>%
  filter(has.abstract == "Yes") %>%
  select(-has.abstract)
  
#renaming columns
names(data.abstract.both.per) <- c("year", "number.has.abstract")

#deselecting the year in the database that counts total number of articles by year
data.by.year.only <- data.by.year %>%
  select(-year)

#binding the columns of total number of articles by year and articles that have abstracts or not
data.abstract.per <- bind_cols(data.by.year.only, data.abstract.both.per)

#renaming the columns to be more indicative of their representation
names(data.abstract.per) <- c("total.articles","year", "number.has.abstract")

#creating the percentage of articles w/ abstracts from the ev data
data.abstract.per <- data.abstract.per %>%
  mutate(per.abstract = (number.has.abstract * 100)/total.articles) %>%
  select(-total.articles, -number.has.abstract)

#renaming the columns in all the dataframes so that they match each other
names(data.abstract.per) <- c("x", "y")

#separating out the different columns that need to be represented to add them to the line graph together later
  ## & renaming the columns so they all match each other
#500 character abstract
data.abstract.per.500 <-thelwall.all.abstracts.graph %>%
  select(year, fiveh.abstracts.per) %>%
  filter(year > 1971, year < 2022)

names(data.abstract.per.500) <- c("x", "y")

#2000 character abstract
data.abstract.per.2000 <- thelwall.all.abstracts.graph %>%
  select(year, twentyh.abstracts.per) %>%
  filter(year > 1971, year < 2022)

names(data.abstract.per.2000) <- c("x", "y")

#1000 character abstract
data.abstract.per.1000 <- thelwall.all.abstracts.graph %>%
  select(year,tenh.abstracts.per)%>%
  filter(year > 1971, year < 2022)

names(data.abstract.per.1000) <- c("x", "y")


#creating a line graph
ggplot(NULL, aes(x,y)) +
  geom_line(data = data.abstract.per, aes(col = "Ecological Validity Items")) + 
  geom_line(data = data.abstract.per.500, aes(col = "All Scopus Items, 500 characters")) + 
  geom_line(data = data.abstract.per.1000, aes(col = "All Scopus Items, 1000 characters")) +
  geom_line(data = data.abstract.per.2000, aes(col = "All Scopus Items, 2000 characters")) + 
  scale_color_manual('Number of characters in abstract', values=c("#F1ECC3", '#C9D8B6', '#57837B', "#515E63")) +
  labs(title= "Percentage of items with abstracts", x = "Year", y = "Percentage of articles with abstracts") +
  ylim(0,100) +
  theme_thesis
```

```{r}
#making a database of reference information associated to the years

#selecting the year information and the reference information
data.reference <- data %>%
  select(year, id, references_001:references_574)

#associating 1 reference to each year from every article in the database
data.reference <- melt(data.reference, id.vars=c("year", "id"), na.rm = TRUE)%>%
  select(-variable)

#making the data lowercase
data.reference$value = tolower(data.reference$value)

#removing the punctuation
data.reference$value <- gsub('[[:punct:] ]+',' ', as.character(data.reference$value))

```

```{r}
#filtering the reference information to see which articles are citing brunswick
data.bruns <- data.reference %>%
  filter(grepl("brunswik", value, ignore.case = TRUE))

data.bruns %>%
  count(id) #80 unique articles

#databse where there is one instance per citation
data.bruns.single <- data.bruns %>%
  group_by(id) %>%
  slice_head() %>%
  ungroup() %>%
  select(-id) %>%
  count(year)

#making a scatter plot of articles that cited brunswik by year
ggplot(data.bruns.single, aes(x=year, y=n)) + 
  geom_point(color = "#57837B") +
  labs(title= "Articles citing 'brunswik' by year", x = "Year", y = "Number of articles") +
  xlim(1970,2022) +
  ylim(0, 10) +
  scale_y_continuous(breaks = c(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10)) +
  theme_thesis
```

```{}
#locating the most cited article
data %>%
  slice_max(cited.by, n = 1)
  #one: doi: 10.1146/annurev.clinpsy.3.022806.091415

#locating the earliest articles (published 1972)
data %>%
  filter(year == 1972)
  #two: doi 10.1037/h0032569 & 10.1207/s15327906mbr0704_2

```


```{}
#first pass at selecting random articles to qualitatively code
#randomly selecting 3 articles
sample_n(data, 3)
  #pass 1, article 1: Bimodal displays improve speech comprehension in environments with multiple speakers, 2003	Human Factors, 10.1518/hfes.45.2.329.27237
    #id: 1379	
  #pass 1, article 2: Looking for the "right" amount to eat at the restaurant: Social influence effects when ordering, 2011	Social Influence, 	10.1080/15534510.2011.632130
    #id: 917	
  #pass 1, article 3: Emotion recognition in temporal lobe epilepsy: A systematic review, 	2015	Neuroscience and Biobehavioral Reviews, 	10.1016/j.neubiorev.2015.05.009
    #id: 403

#selecting one article that is in the top 25% range of citations
  #determining what the top 25% of citations is: 
quantile(data$cited.by, na.rm=TRUE) #result, top 25% is more than 32 citations

#filtering to make a data set of articles cited more than 32 times
data.top.75per <- data %>%
  filter(cited.by > 32)

#selecting one random article in the top 25% citation range
sample_n(data.top.75per,1)
  #pass 1, article 4: Initial examination of the validity and reliability of the female photographic figure rating scale for body image assessment, 2008, 	Personality and Individual Differences	44	8	1752	1761	119	10.1016/j.paid.2008.02.002

#making a data base of articles from 1972 to 1990 inclusive & randomly selecting one article
data.72.90 <- data %>%
  filter(year < 1991) %>%
  sample_n(1)
  #pass 1, article 5: Tidig stimulering av barn: Internationella stromningar, 1983 Nordisk Psykologi, 10.1080/00291463.1983.10636862 
    #this article is in Swedish and I can't find an English translation, since I don't speak Swedish I filtered for another article to analyze in its stead
  #pass 1.5: Interracial imitation at a summer camp, 1975, Journal of Personality and Social Psychology. 10.1037/0022-3514.32.6.1099

#making a data base of articles from 1991 to 2005 inclusive & randomly selecting one article
data.91.05 <- data %>%
  filter(year < 2006 & year > 1990) %>%
  sample_n(1) 
  #pass 1, article 6: Ecological validity of laboratory studies of videopoker gaming, 1991, Journal of Gambling Studies. 10.1007/BF01014526

#making a database of articles from 2006 to 2021 inclusive & randomly selecting one article
data.06.21 <- data %>%
  filter(year > 2005) %>%
  sample_n(1)
  #pass 1, article 7: Predicting advertising effectiveness by facial expressions in response to amusing persuasive stimuli, 2014, Journal of Neuroscience, Psychology, and Economics. 10.1037/npe0000012
```

```{}
#second pass at selecting random articles to qualitatively code

#randomly selecting 3 articles
set.seed(132)
sample_n(data, 3)
  #pass 2, article 1: 10.1016/j.concog.2010.03.011
  #pass 2, article 2: 10.1017/S0954579421000791
  #pass 2, article 3: 10.3389/fpsyg.2021.684012

#selecting one random article in the top 25% citation range
set.seed(132)
sample_n(data.top.75per,1)
  #pass 2, article 4: 	10.1037/0012-1649.23.1.49

#selecting an article from 1972-1990
set.seed(132)
data.72.90 %>%
  sample_n(1)
  #pass 2: article 5: 10.1177/0261927X8300200102

#making a database of articles from 1991 to 2005 inclusive & randomly selecting one article
set.seed(132)
data.91.05 %>%
  sample_n(1) 
  #pass 2, article 6: 10.1037/0033-2909.112.3.411

#making a database of articles from 2006 to 2021 inclusive & randomly selecting one article
set.seed(132)
data.06.21 %>%
  sample_n(1)
  #pass 2, article 7: 	10.1016/j.tics.2018.10.001
```

```{r}

```

