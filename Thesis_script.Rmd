---
title: "Thesis Notebook"
output: html_notebook
---

```{}
THINGS THAT DON'T REALLY WORK ANYMORE AND I'LL PROBABLY END UP DELETING, BUT SHOULD STICK AROUND FOR NOW
#loading the csv files and then joining them in to one data base
  ##converting blanks in to NA

#american journal of psychology
ajp_01 <- read.csv("ajp_01.csv", na.strings = c("", "NA"))
ajp_02 <- read.csv("ajp_02.csv", na.strings = c("", "NA"))

#psychological bulletin
pb_01 <- read.csv("pb_01.csv", na.strings = c("", "NA"))
pb_02 <- read.csv("pb_02.csv", na.strings = c("", "NA"))
pb_03 <- read.csv("pb_03.csv", na.strings = c("", "NA"))
pb_04 <- read.csv("pb_04.csv", na.strings = c("", "NA"))

#perspectives on psychological science
pps_01 <- read.csv("pps_01.csv", na.strings = c("", "NA"))

#psychological review
pr_01 <- read.csv("pr_01.csv", na.strings = c("", "NA"))
pr_02 <- read.csv("pr_02.csv", na.strings = c("", "NA"))
pr_03 <- read.csv("pr_03.csv", na.strings = c("", "NA"))

#loading the packages all together
data.loaded <- rbind(ajp_01, ajp_02,
                       pb_01, pb_02, pb_03, pb_04,
                       pps_01,
                       pr_01, pr_02, pr_03)

#creating a .csv file from table with all of the databases bound together to keep environment clean through the project
write_csv(data.loaded, "data.loaded.csv")

---
data.title <- data %>%
  filter(grepl("ecological validity|ecologically valid", title, ignore.case = TRUE))

data.abstract <- data %>%
  filter(grepl("ecological validity|ecologically valid", abstract, ignore.case = TRUE))

data.author.keywords <- data %>%
  filter(grepl("Necological validity|ecologically valid", author.keywords, ignore.case = TRUE))

data.index.keywords <- data %>%
  filter(grepl("ecological validity|ecologically valid", index.keywords, ignore.case = TRUE))
  
---
#BINDING WITH THE EXTRA COLUMNS
#putting all of the categories back together 
data.ev <- do.call("rbind", list(data.title, data.abstract, data.author.keywords, data.index.keywords))

#deleting any articles that might have replicated
data.ev <- data.ev %>%
  distinct(id, .keep_all = TRUE)
---
```

libraries
  load once per session
```{r message=FALSE}
#loading the necesary packages for this code
library(dplyr)
library(tidyr)
library(ggplot2)
library(readr)
library(stringr)
library(skimr)
library(splitstackshape)
library(janitor)
library(jcolors)
library(wordcloud)
library(wordcloud2)
library(tm)
library(data.table)
```


```{r}
#loading the csv files and then joining them in to one data base
  ##converting blanks in to NA

#csv file with data from 1972-2017
ev_1 <- read.csv("ev_1.csv", na.strings = c("", "NA"))

#csv file with data from 2018-2020
ev_2 <- read.csv("ev_2.csv", na.strings = c("", "NA"))

#csv file with data from 2021
ev_3 <- read.csv("ev_3.csv", na.strings = c("", "NA"))

#loading the packages all together
data.loaded <- rbind(ev_1, ev_2, ev_3)

#creating a .csv file from table with all of the databases bound together to keep environment clean through the project
write_csv(data.loaded, "data.loaded.csv")
```


cleaning
  load once per session
```{r}
#loading the csv file with all the databases in to a data frame
data.original <- read.csv("data.loaded.csv")

#cleaning the csv file for better usage in this project

#discarding the columns that won't be used for the project
data.original <- data.original %>%
 select(-Art..No., -Molecular.Sequence.Numbers, -Chemicals.CAS,-Tradenames, -Manufacturers, -Funding.Text.1, -Funding.Text.2, -Funding.Text.3, -Funding.Text.4, -Funding.Text.5, -Funding.Text.6, -Funding.Text.7, -Funding.Text.8, -Funding.Text.9, -Funding.Text.10, -Correspondence.Address,  -Abbreviated.Source.Title)

#reaming the columns for a better workflow
names(data.original) <- c("authors", "author.id", "title", "year", "source.title", "volume", "issue", "page.start", "page.end", "page.count", "cited.by", "doi", "link", "affiliations", "author.w.affiliations", "abstract", "author.keywords", "index.keywords", "funding.details", "references", "editors", "sponsors", "publisher", "conference.name", "conference.date", "conference.location", "conference.code", "issn", "isbn", "coden", "pubmed.id", "original.language", "document.type", "publication.stage", "open.access", "source", "eid")

#discarding any repeated rows (ie: documents) that might have shown up
data <- unique(data.original)

#checking which columns use "[No x available]" format to replace that message with "NA" instead for better workflow later
  ##those columns that use it have exact language listed in comments next to it
data %>% filter(grepl("available]", authors)) #[No author name available]
data %>% filter(grepl("available]", author.id)) #[No author id available]
data %>% filter(grepl("available]", title))
data %>% filter(grepl("available]", year))
data %>% filter(grepl("available]", source.title))
data %>% filter(grepl("available]", volume))
data %>% filter(grepl("available]", issue))
data %>% filter(grepl("available]", page.start))
data %>% filter(grepl("available]", page.end))
data %>% filter(grepl("available]", page.count))
data %>% filter(grepl("available]", cited.by))
data %>% filter(grepl("available]", doi))
data %>% filter(grepl("available]", link))
data %>% filter(grepl("available]", affiliations))
data %>% filter(grepl("available]", author.w.affiliations))
data %>% filter(grepl("available]", abstract)) #[No abstract available]
data %>% filter(grepl("available]", author.keywords))
data %>% filter(grepl("available]", index.keywords))
data %>% filter(grepl("available]", funding.details))
data %>% filter(grepl("available]", references))
data %>% filter(grepl("available]", editors))
data %>% filter(grepl("available]", sponsors))
data %>% filter(grepl("available]", publisher))
data %>% filter(grepl("available]", conference.name))
data %>% filter(grepl("available]", conference.date))
data %>% filter(grepl("available]", conference.location))
data %>% filter(grepl("available]", conference.code))
data %>% filter(grepl("available]", issn))
data %>% filter(grepl("available]", isbn))
data %>% filter(grepl("available]", coden))
data %>% filter(grepl("available]", pubmed.id))
data %>% filter(grepl("available]", original.language))
data %>% filter(grepl("available]", document.type))
data %>% filter(grepl("available]", publication.stage))
data %>% filter(grepl("available]", open.access))
data %>% filter(grepl("available]", source))
data %>% filter(grepl("available]", eid))

#replacing text about missing data for NA instead in the columns that use text to indicate missing information
data <- data %>%
  mutate(authors = na_if(authors, "[No author name available]")) %>%
  mutate(author.id = na_if(author.id, "[No author id available]")) %>%
  mutate(abstract = na_if(abstract, "[No abstract available]"))

##checking if any columns are totally empty
  ##almost totally empty: editors (5 not empty), sponsors (4 not empty), conference.name (9 not empty), conference.date (9 not empty), conference location (2 not empty), conference code (8 not empty), page.count (13 not empty)
colSums(is.na(data))

#discarding columns that are totally empty or that are empty except for one row, as they will not provide useful information in the aggregate
  #adding an article ID to each column
data <- data %>%
 select(- editors, -sponsors, -conference.name, -conference.date, -conference.location, -conference.code, -isbn, -page.count) %>%
  mutate(id = row_number())
```

obtaining information about what journals are in the database, what years are in the database, and 
```{r}
#obtaining the articles that are represented in the database
  #total of 719 journals represented
journals <- unique(data[c("source.title")])

#obtaining the years that are represented in the database: 1972 - 2021
min(data$year)
max(data$year)

#finding the earliest instance by each journal
journal.first.instance <- data %>%
  group_by(source.title) %>%
  filter(year == min(year)) %>%
  select(year, source.title) %>%
  distinct(year)

write_csv(journal.first.instance, "journals_by_year.csv")
```

```{r}
#creating an aesthetic for the visualizations
```


```{r}
theme_thesis <- theme_classic()
```


```{r}
#making a graph of what year journals began to publish about ecological validity

#counting number of journals with articles first published by year
journal.first.instance <- journal.first.instance %>%
  ungroup() %>%
  select(year) %>%
  count(year)

  ggplot(journal.first.instance) +
  geom_line(aes(x=year, y=n)) +  
    scale_x_continuous(breaks = c(1970, 1975, 1980, 1985, 1990, 1995, 2000, 2005, 2010, 2015, 2020)) +
    labs(title= "First year journals published articles about Ecological Validity", x = "Number of journals", y = "Year first EV article was published") +
    theme_thesis

```

```{r}
#graph of distribution by year

#filtering just the year data and counting how many articles there are per year
data.by.year <- data %>%
  select(year) %>%
  count(year)

#creating the graph of the counts per year
ggplot(data.by.year) +
  geom_line(aes(x=year, y=n)) +
  scale_x_continuous(breaks = c(1970, 1975, 1980, 1985, 1990, 1995, 2000, 2005, 2010, 2015, 2020)) +
  scale_y_continuous(breaks = c(0, 25, 50, 75, 100, 125, 150, 175, 200, 225, 250)) +
  labs(title= "Number of articles about EV per year in all journals", x = "Year", y = "Number of journals") +
  theme_thesis

```

```{r}
#histogram of how many citations EV items recieved
  ##277 rows were removed by R for "containign non-finite values"
  ## bin width of 30
ggplot(data, aes(x=cited.by)) +
  geom_histogram() +
  labs(title= "Histogram of the number of citations recieved by EV items", x = "Number of citations", y = "Number of articles") +
  theme_thesis
```


```{r}
#checking how many articles are missing abstracts
  #total articles missing abstracts = 
data.no.abstract <- data %>%
  filter(is.na(abstract)) %>% #filtering those rows in abstract that have NA
  mutate(has.abstract = "No")

data.yes.abstract <- data %>%
  filter(!(id %in% data.no.abstract$id)) %>% #filtering out those articles already categorized as NA
  mutate(has.abstract = "Yes")

data.abstract.both <- rbind(data.no.abstract, data.yes.abstract)

#stacked bar graph of whether different kinds of items have abstracts listed or not

#FIGURE OUT HOW TO TILT THE TEXT
ggplot(data.abstract.both, aes(x=document.type, fill=has.abstract)) +
  geom_bar()+
  labs(title= "Distribution of document types about EV and wether they have abstracts", x = "Kinds of documents", y = "Number of documents") +
  guides(fill = guide_legend(title = "Has Abstract")) +
  #theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  theme_thesis
```

```{r}
#bar graph of whether the items have abstracts or not
data.abstract.both <- data.abstract.both %>%
  count(has.abstract) %>%
  mutate(per = (100*n)/2650)

#rounding the percentage
data.abstract.both$per <- round(data.abstract.both$per)

#making the graph
ggplot(data.abstract.both, aes(x="", y = per, fill = has.abstract)) +
  geom_bar(stat="identity", width=1) +
  coord_polar("y", start=0) + 
  theme_void()+
  geom_text(aes(label = paste0(per,"%")), position = position_stack(vjust=0.5)) +
    guides(fill = guide_legend(title = "Has Abstract")) +
  labs(title = "Percentages of items with or without abstracts")

```























