---
title: "Thesis Notebook"
output: html_notebook
---

```{}
THINGS THAT DON'T REALLY WORK ANYMORE AND I'LL PROBABLY END UP DELETING, BUT SHOULD STICK AROUND FOR NOW
#loading the csv files and then joining them in to one data base
  ##converting blanks in to NA

#american journal of psychology
ajp_01 <- read.csv("ajp_01.csv", na.strings = c("", "NA"))
ajp_02 <- read.csv("ajp_02.csv", na.strings = c("", "NA"))

#psychological bulletin
pb_01 <- read.csv("pb_01.csv", na.strings = c("", "NA"))
pb_02 <- read.csv("pb_02.csv", na.strings = c("", "NA"))
pb_03 <- read.csv("pb_03.csv", na.strings = c("", "NA"))
pb_04 <- read.csv("pb_04.csv", na.strings = c("", "NA"))

#perspectives on psychological science
pps_01 <- read.csv("pps_01.csv", na.strings = c("", "NA"))

#psychological review
pr_01 <- read.csv("pr_01.csv", na.strings = c("", "NA"))
pr_02 <- read.csv("pr_02.csv", na.strings = c("", "NA"))
pr_03 <- read.csv("pr_03.csv", na.strings = c("", "NA"))

#loading the packages all together
data.loaded <- rbind(ajp_01, ajp_02,
                       pb_01, pb_02, pb_03, pb_04,
                       pps_01,
                       pr_01, pr_02, pr_03)

#creating a .csv file from table with all of the databases bound together to keep environment clean through the project
write_csv(data.loaded, "data.loaded.csv")

---
data.title <- data %>%
  filter(grepl("ecological validity|ecologically valid", title, ignore.case = TRUE))

data.abstract <- data %>%
  filter(grepl("ecological validity|ecologically valid", abstract, ignore.case = TRUE))

data.author.keywords <- data %>%
  filter(grepl("Necological validity|ecologically valid", author.keywords, ignore.case = TRUE))

data.index.keywords <- data %>%
  filter(grepl("ecological validity|ecologically valid", index.keywords, ignore.case = TRUE))
  
---
#BINDING WITH THE EXTRA COLUMNS
#putting all of the categories back together 
data.ev <- do.call("rbind", list(data.title, data.abstract, data.author.keywords, data.index.keywords))

#deleting any articles that might have replicated
data.ev <- data.ev %>%
  distinct(id, .keep_all = TRUE)
---
```

libraries
  load once per session
```{r message=FALSE}
#loading the necesary packages for this code
library(dplyr)
library(tidyr)
library(ggplot2)
library(readr)
library(stringr)
library(skimr)
library(splitstackshape)
library(janitor)
library(jcolors)
library(wordcloud)
library(wordcloud2)
library(tm)
library(data.table)
library(utils)
library(reshape2)

#quanteda packages
library(Rcpp) #rcpp has to load before quanteda
library(quanteda)
library(readtext)
library(devtools)
library(quanteda.textmodels)

library(spacyr)
library(newsmap)
```


```{}
#loading the csv files and then joining them in to one data base
  ##converting blanks in to NA

#csv file with data from 1972-2017
ev_1 <- read.csv("ev_1.csv", na.strings = c("", "NA"))

#csv file with data from 2018-2020
ev_2 <- read.csv("ev_2.csv", na.strings = c("", "NA"))

#csv file with data from 2021
ev_3 <- read.csv("ev_3.csv", na.strings = c("", "NA"))

#loading the packages all together
data.loaded <- rbind(ev_1, ev_2, ev_3)

#creating a .csv file from table with all of the databases bound together to keep environment clean through the project
write_csv(data.loaded, "data.loaded.csv")
```


cleaning
  load once per session
```{r}
#loading the csv file with all the databases in to a data frame
data.original <- read.csv("data.loaded.csv")

#cleaning the csv file for better usage in this project

#discarding the columns that won't be used for the project
data.original <- data.original %>%
 select(-Art..No., -Molecular.Sequence.Numbers, -Chemicals.CAS,-Tradenames, -Manufacturers, -Funding.Text.1, -Funding.Text.2, -Funding.Text.3, -Funding.Text.4, -Funding.Text.5, -Funding.Text.6, -Funding.Text.7, -Funding.Text.8, -Funding.Text.9, -Funding.Text.10, -Correspondence.Address,  -Abbreviated.Source.Title)

#reaming the columns for a better workflow
names(data.original) <- c("authors", "author.id", "title", "year", "source.title", "volume", "issue", "page.start", "page.end", "page.count", "cited.by", "doi", "link", "affiliations", "author.w.affiliations", "abstract", "author.keywords", "index.keywords", "funding.details", "references", "editors", "sponsors", "publisher", "conference.name", "conference.date", "conference.location", "conference.code", "issn", "isbn", "coden", "pubmed.id", "original.language", "document.type", "publication.stage", "open.access", "source", "eid")

#discarding any repeated rows (ie: documents) that might have shown up
data <- unique(data.original)

#checking which columns use "[No x available]" format to replace that message with "NA" instead for better workflow later
  ##those columns that use it have exact language listed in comments next to it
data %>% filter(grepl("available]", authors)) #[No author name available]
data %>% filter(grepl("available]", author.id)) #[No author id available]
data %>% filter(grepl("available]", title))
data %>% filter(grepl("available]", year))
data %>% filter(grepl("available]", source.title))
data %>% filter(grepl("available]", volume))
data %>% filter(grepl("available]", issue))
data %>% filter(grepl("available]", page.start))
data %>% filter(grepl("available]", page.end))
data %>% filter(grepl("available]", page.count))
data %>% filter(grepl("available]", cited.by))
data %>% filter(grepl("available]", doi))
data %>% filter(grepl("available]", link))
data %>% filter(grepl("available]", affiliations))
data %>% filter(grepl("available]", author.w.affiliations))
data %>% filter(grepl("available]", abstract)) #[No abstract available]
data %>% filter(grepl("available]", author.keywords))
data %>% filter(grepl("available]", index.keywords))
data %>% filter(grepl("available]", funding.details))
data %>% filter(grepl("available]", references))
data %>% filter(grepl("available]", editors))
data %>% filter(grepl("available]", sponsors))
data %>% filter(grepl("available]", publisher))
data %>% filter(grepl("available]", conference.name))
data %>% filter(grepl("available]", conference.date))
data %>% filter(grepl("available]", conference.location))
data %>% filter(grepl("available]", conference.code))
data %>% filter(grepl("available]", issn))
data %>% filter(grepl("available]", isbn))
data %>% filter(grepl("available]", coden))
data %>% filter(grepl("available]", pubmed.id))
data %>% filter(grepl("available]", original.language))
data %>% filter(grepl("available]", document.type))
data %>% filter(grepl("available]", publication.stage))
data %>% filter(grepl("available]", open.access))
data %>% filter(grepl("available]", source))
data %>% filter(grepl("available]", eid))

#replacing text about missing data for NA instead in the columns that use text to indicate missing information
data <- data %>%
  mutate(authors = na_if(authors, "[No author name available]")) %>%
  mutate(author.id = na_if(author.id, "[No author id available]")) %>%
  mutate(abstract = na_if(abstract, "[No abstract available]"))

##checking if any columns are totally empty
  ##almost totally empty: editors (5 not empty), sponsors (4 not empty), conference.name (9 not empty), conference.date (9 not empty), conference location (2 not empty), conference code (8 not empty), page.count (13 not empty)
colSums(is.na(data))

#discarding columns that are totally empty or that are empty except for one row, as they will not provide useful information in the aggregate
  #adding an article ID to each column
data <- data %>%
 select(- editors, -sponsors, -conference.name, -conference.date, -conference.location, -conference.code, -isbn, -page.count) %>%
  mutate(id = row_number())
```

```{r} 
#STARTED GETTING AN ERROR HERE ONCE I SPLIT THE DATA, SO I MOVED IT UP
#obtaining the articles that are represented in the database
  #total of 719 journals represented
journals <- unique(data[c("source.title")])

#obtaining the years that are represented in the database: 1972 - 2021
min(data$year)
max(data$year)

#finding the earliest instance by each journal
journal.first.instance <- data %>%
  group_by(source.title) %>%
  filter(year == min(year)) %>%
  select(year, source.title) %>%
  distinct(year)

write_csv(journal.first.instance, "journals_by_year.csv")
```

```{r}
#giving each author.id, author name, affilication, index.keywords, references, and author.keywords a separate column for each piece of distinct data
data <- cSplit(data, "author.id", sep=";")

data <-  cSplit(data, "authors", sep=",")
  
data <-  cSplit(data, "affiliations", sep=";")

data <- cSplit(data, "index.keywords", sep=";")
  
data <- cSplit(data, "references", sep=";")

data <- cSplit(data, "author.keywords", sep=";")
```


```{r}
#creating an aesthetic for the visualizations
theme_thesis <- theme_classic()
```


```{r}
#making a graph of what year journals began to publish about ecological validity

#counting number of journals with articles first published by year
journal.first.instance <- journal.first.instance %>%
  ungroup() %>%
  select(year) %>%
  count(year)

  ggplot(journal.first.instance) +
  geom_line(aes(x=year, y=n)) +  
    scale_x_continuous(breaks = c(1970, 1975, 1980, 1985, 1990, 1995, 2000, 2005, 2010, 2015, 2020)) +
    labs(title= "First year journals published articles about Ecological Validity", x = "Number of journals", y = "Year first EV article was published") +
    theme_thesis

```

```{r}
#graph of distribution by year

#filtering just the year data and counting how many articles there are per year
data.by.year <- data %>%
  select(year) %>%
  count(year)

#creating the graph of the counts per year
ggplot(data.by.year) +
  geom_line(aes(x=year, y=n)) +
  scale_x_continuous(breaks = c(1970, 1975, 1980, 1985, 1990, 1995, 2000, 2005, 2010, 2015, 2020)) +
  scale_y_continuous(breaks = c(0, 25, 50, 75, 100, 125, 150, 175, 200, 225, 250)) +
  labs(title= "Number of articles about EV per year in all journals", x = "Year", y = "Number of journals") +
  theme_thesis

```

```{r}
#histogram of how many citations EV items recieved
  ##277 rows were removed by R for "containign non-finite values"
  ## bin width of 30
ggplot(data, aes(x=cited.by)) +
  geom_histogram() +
  labs(title= "Histogram of the number of citations recieved by EV items", x = "Number of citations", y = "Number of articles") +
  theme_thesis
```


```{r}
#checking how many articles are missing abstracts
  #total articles missing abstracts = 
data.no.abstract <- data %>%
  filter(is.na(abstract)) %>% #filtering those rows in abstract that have NA
  mutate(has.abstract = "No")

data.yes.abstract <- data %>%
  filter(!(id %in% data.no.abstract$id)) %>% #filtering out those articles already categorized as NA
  mutate(has.abstract = "Yes")

data.abstract.both <- rbind(data.no.abstract, data.yes.abstract)

#stacked bar graph of whether different kinds of items have abstracts listed or not

#FIGURE OUT HOW TO TILT THE TEXT
ggplot(data.abstract.both, aes(x=document.type, fill=has.abstract)) +
  geom_bar()+
  labs(title= "Distribution of document types about EV and wether they have abstracts", x = "Kinds of documents", y = "Number of documents") +
  guides(fill = guide_legend(title = "Has Abstract")) +
  #theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  theme_thesis
```

```{r}
#pie graph of whether the items have abstracts or not
data.abstract.both <- data.abstract.both %>%
  count(has.abstract) %>%
  mutate(per = (100*n)/2650)

#rounding the percentage
data.abstract.both$per <- round(data.abstract.both$per)

#making the graph
ggplot(data.abstract.both, aes(x="", y = per, fill = has.abstract)) +
  geom_bar(stat="identity", width=1) +
  coord_polar("y", start=0) + 
  theme_void()+
  geom_text(aes(label = paste0(per,"%")), position = position_stack(vjust=0.5)) +
    guides(fill = guide_legend(title = "Has Abstract")) +
  labs(title = "Percentages of items with or without abstracts")

```

```{r}
#trying to use quanteda to make wordclouds
index.keywords <- data %>%
  select(index.keywords_01:index.keywords_90, id)

#collapsing the row of columns 01-90 in to one column
index.keywords <-  melt(index.keywords, id.vars=c("id"), na.rm = TRUE) %>%
  select(-variable)

#making everything lowercase
index.keywords$value = tolower(index.keywords$value)

#taking away punctuation
index.keywords$value <- gsub('[[:punct:] ]+',' ', as.character(index.keywords$value))
```


```{r}
#making a corpus of the abstracts of the documents
data.abstract <- data

#converting to lowercase and removing punctuation
data.abstract$abstract <- gsub('[[:punct:] ]+',' ', as.character(data.abstract$abstract))
data.abstract$abstract = tolower(data.abstract$abstract)
  
corp_abstract <- corpus(data, text_field = "abstract")

#adding id number to the text information
docnames(corp_abstract) <- data$id 
print(corp_abstract)
```


```{r}
#able to work with different subsets with the following code: 
  ## can check different analysis by year
corp_recent <- corpus_subset(corp, Year >= 1990)
ndoc(corp_recent)

##You can see how keywords are used in the actual contexts in a concordance view produced by kwic().
  #https://tutorials.quanteda.io/basic-operations/tokens/kwic/
kw_immig <- kwic(toks, pattern =  "immig*")
head(kw_immig, 10)

#You can remove tokens that you are not interested in using tokens_select(). Usually we remove function words (grammatical words) that have little or no substantive meaning in pre-processing. stopwords() returns a pre-defined list of function words.

```


```{}
#information about number of articles indexed in scopus by year, sourced from the extra materials of:
  #Thelwall, M. (2022) Scopus 1900-2020: Growth in articles, abstracts, countries, fields, and journals. Quantitative Science Studies. Advance Publication. https://doi.org/10.1162/qss_a_00177

scopus.psych <- data.frame(year = c(1900,	1901,	1902,	1903,	1904,	1905,	1906,	1907,	1908,	1909,	1910,	1911,	1912,	1913,	1914,	1915,	1916,	1917,	1918,	1919,	1920,	1921,	1922,	1923,	1924,	1925,	1926,	1927,	1928,	1929,	1930,	1931,	1932,	1933,	1934,	1935,	1936,	1937,	1938,	1939,	1940,	1941,	1942,	1943,	1944,	1945,	1946,	1947,	1948,	1949,	1950,	1951,	1952,	1953,	1954,	1955,	1956,	1957,	1958,	1959,	1960,	1961,	1962,	1963,	1964,	1965,	1966,	1967,	1968,	1969,	1970,	1971,	1972,	1973,	1974,	1975,	1976,	1977,	1978,	1979,	1980,	1981,	1982,	1983,	1984,	1985,	1986,	1987,	1988,	1989,	1990,	1991,	1992,	1993,	1994,	1995,	1996,	1997,	1998,	1999,	2000,	2001,	2002,	2003,	2004,	2005,	2006,	2007,	2008,	2009,	2010,	2011,	2012,	2013,	2014,	2015,	2016,	2017,	2018,	2019,	2020),
                           articles = c(44,	33,	44,	32,	69,	57,	53,	54,	93,	84,	81,	92,	100,	108,	119,	104,	108,	105,	84,	104,	95,	62,	109,	105,	96,	80,	138,	134,	151,	123,	149,	133,	142,	137,	138,	141,	161,	232,	218,	179,	186,	184,	145,	182,	171,	186,	171,	192,	219,	294,	368,	362,	407,	444,	422,	351,	340,	325,	369,	359,	304,	341,	417,	437,	537,	1054,	1079,	1289,	1436,	1163,	1470,	1526,	1478,	1855,	2025,	1942,	2101,	2139,	2411,	2346,	2490,	2659,	2806,	2804,	2711,	2799,	2748,	2789,	3009,	3007,	3347,	3352,	3551,	3490,	3540,	3815,	5051,	4957,	5260,	5189,	5363,	5521,	5245,	4746,	5531,	5331,	6832,	7873,	8213,	7032,	7448,	7778,	8246,	8949,	10238,	10324,	11584,	12028,	12092,	13691,	15634))
```



```{r}
#loading the csv file on number of articles about number of articles scopus has indexed for each year
psych.articles.year <- read.csv("articles.by.year.csv", na.strings = c("", "NA"))

```

```{r}
#making a graph of number of journals indexed by Scopus under the category of psychology
  ggplot(psych.articles.year, aes(x = year, y = article.count)) +
  geom_line() +  
    scale_x_continuous(breaks = c(1940, 1945, 1950, 1955, 1960, 1965, 1970, 1975, 1980, 1985, 1990, 1995, 2000, 2005, 2010, 2015, 2020)) +
    labs(title= "Number of journals indexed by Scopus under the category of psychology", x = "Year", y = "Number of journals") +
    theme_thesis

```



```{r}
#loading information from Scopus representative about the number of articles per particular journal that are uploaded to scopus January-February 2022

scopus_journal_counts <- read.csv("scopus_journal_counts.csv", na.strings = c("", "NA"))

scopus_source_list <- read.csv("scopus_source_list.csv", na.strings = c("", "NA"))
```


```{r}
#locating the most cited article
data %>%
  slice_max(cited.by, n = 1)
  #one: doi: 10.1146/annurev.clinpsy.3.022806.091415

#locating the earliest articles (published 1972)
data %>%
  filter(year == 1972)
  #two: doi 10.1037/h0032569 & 10.1207/s15327906mbr0704_2

```



```{r}
#first pass at selecting random articles to qualitatively code
#randomly selecting 3 articles
sample_n(data, 3)
  #pass 1, article 1: Bimodal displays improve speech comprehension in environments with multiple speakers, 2003	Human Factors, 10.1518/hfes.45.2.329.27237
    #id: 1379	
  #pass 1, article 2: Looking for the "right" amount to eat at the restaurant: Social influence effects when ordering, 2011	Social Influence, 	10.1080/15534510.2011.632130
    #id: 917	
  #pass 1, article 3: Emotion recognition in temporal lobe epilepsy: A systematic review, 	2015	Neuroscience and Biobehavioral Reviews, 	10.1016/j.neubiorev.2015.05.009
    #id: 403

#selecting one article that is in the top 25% range of citations
  #determining what the top 25% of citations is: 
quantile(data$cited.by, na.rm=TRUE) #result, top 25% is more than 32 citations

#filtering to make a data set of articles cited more than 32 times
data.top.75per <- data %>%
  filter(cited.by > 32)

#selecting one random article in the top 25% citation range
sample_n(data.top.75per,1)
  #pass 1, article 4: Initial examination of the validity and reliability of the female photographic figure rating scale for body image assessment, 2008, 	Personality and Individual Differences	44	8	1752	1761	119	10.1016/j.paid.2008.02.002

#making a data base of articles from 1972 to 1990 inclusive & randomly selecting one article
data.72.90 <- data %>%
  filter(year < 1991) %>%
  sample_n(1)
  #pass 1, article 5: Tidig stimulering av barn: Internationella stromningar, 1983 Nordisk Psykologi, 10.1080/00291463.1983.10636862 
    #this article is in Swedish and I can't find an English translation, since I don't speak Swedish I filtered for another article to analyze in its stead
  #pass 1.5: Interracial imitation at a summer camp, 1975, Journal of Personality and Social Psychology. 10.1037/0022-3514.32.6.1099

#making a data base of articles from 1991 to 2005 inclusive & randomly selecting one article
data.91.05 <- data %>%
  filter(year < 2006 & year > 1990) %>%
  sample_n(1)
  #pass 1, article 6: Ecological validity of laboratory studies of videopoker gaming, 1991, Journal of Gambling Studies. 10.1007/BF01014526

#making a database of articles from 2006 to 2021 inclusive & randomly selecting one article
data.06.21 <- data %>%
  filter(year > 2005) %>%
  sample_n(1)
  #pass 1, article 7: Predicting advertising effectiveness by facial expressions in response to amusing persuasive stimuli, 2014, Journal of Neuroscience, Psychology, and Economics. 10.1037/npe0000012
```




